{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from torch.utils.data import TensorDataset,Dataset,DataLoader,random_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Load the Excel file-----------------------\n",
    "def load_data(file_path, sheet_name):\n",
    "    sheet_name = sheet_name\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    index_tensor = (df['A_position_counted_from_5_end_of_gRNA'] + 20).astype(int) \n",
    "    return df, index_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class FeatureProcessor:\n",
    "    def __init__(self, file_path='D:/01IscBML/', file_name = 'train_del_keep1700.xlsx'):\n",
    "        self.category_map = {'A': 0, 'T': 1, 'C': 2, 'G': 3}\n",
    "        self.data_path = file_path + file_name\n",
    "        self.df, self.index_tensor = self.load_data(self.data_path)  # load the df and index tensor as self attributes\n",
    "        self.combined_features, self.sequence_length = self.feature_processing()\n",
    "        self.labels_tensor = self.label()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.shape[0])\n",
    "\n",
    "    def load_data(self, data_path):\n",
    "        df = pd.read_excel(data_path)\n",
    "        index_tensor = (df['A_position_counted_from_5_end_of_gRNA'] + 20).astype(int)-1 # Warning: 0-indexed\n",
    "        return df, index_tensor\n",
    "\n",
    "    def sequence_to_one_hot(self, sequence, categories=\"ATCG\"):\n",
    "        category_map = {char: idx for idx, char in enumerate(categories)}\n",
    "        one_hot = []\n",
    "        for char in sequence:\n",
    "            one_hot_char = [0] * len(categories)\n",
    "            if char in category_map:\n",
    "                one_hot_char[category_map[char]] = 1\n",
    "            one_hot.append(one_hot_char)\n",
    "        return one_hot\n",
    "\n",
    "    def feature_processing(self):\n",
    "        sequence_data = self.df['Target_Site_sequence']\n",
    "        sequence_length = len(sequence_data.iloc[0])\n",
    "        one_hot_features = sequence_data.apply(self.sequence_to_one_hot)\n",
    "\n",
    "        features_tensor = torch.tensor(list(one_hot_features), dtype=torch.float32)\n",
    "\n",
    "        index_one_hot_features = []\n",
    "        for idx in self.index_tensor:\n",
    "            index_one_hot = [0] * sequence_length\n",
    "            if 0 <= idx < sequence_length:\n",
    "                index_one_hot[idx] = 1\n",
    "            index_one_hot_features.append(index_one_hot)\n",
    "\n",
    "        index_one_hot_tensor = torch.tensor(index_one_hot_features, dtype=torch.float32).unsqueeze(2)\n",
    "\n",
    "        combined_features = torch.cat((features_tensor, index_one_hot_tensor), dim=2)\n",
    "        return combined_features, sequence_length\n",
    "    \n",
    "    def label(self):\n",
    "        #---------------------Prepare labels tensor-----------------------\n",
    "        labels_tensor = torch.tensor(self.df['a-to-g(%)'].values/100, dtype=torch.float32).unsqueeze(1)  # Add a dimension for labels\n",
    "        return labels_tensor\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Target_Site_sequence'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Target_Site_sequence'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m label_data \u001b[38;5;241m=\u001b[39m \u001b[43mFeatureProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:/01IscBML/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_del_keep1700.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlabels_tensor\n\u001b[0;32m      2\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m FeatureProcessor(file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/01IscBML/\u001b[39m\u001b[38;5;124m'\u001b[39m,file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_del_keep1700.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcombined_features\n",
      "Cell \u001b[1;32mIn[21], line 9\u001b[0m, in \u001b[0;36mFeatureProcessor.__init__\u001b[1;34m(self, file_path, file_name)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path \u001b[38;5;241m=\u001b[39m file_path \u001b[38;5;241m+\u001b[39m file_name\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path)  \u001b[38;5;66;03m# load the df and index tensor as self attributes\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombined_features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel()\n",
      "Cell \u001b[1;32mIn[21], line 31\u001b[0m, in \u001b[0;36mFeatureProcessor.feature_processing\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_processing\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 31\u001b[0m     sequence_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTarget_Site_sequence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     32\u001b[0m     sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sequence_data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     33\u001b[0m     one_hot_features \u001b[38;5;241m=\u001b[39m sequence_data\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_to_one_hot)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Target_Site_sequence'"
     ]
    }
   ],
   "source": [
    "label_data = FeatureProcessor(file_path='D:/01IscBML/', file_name='train_del_keep1700.xlsx').labels_tensor\n",
    "combined_features = FeatureProcessor(file_path='D:/01IscBML/',file_name='train_del_keep1700.xlsx').combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = FeatureProcessor(file_path='D:/01IscBML/').sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------Prepare labels tensor-----------------------\n",
    "# labels_tensor = torch.tensor(df['a-to-g(%)'].values/100, dtype=torch.float32).unsqueeze(1)  # Add a dimension for labels\n",
    "\n",
    "#---------------------Create dataset and dataloader-----------------\n",
    "dataset = TensorDataset(combined_features, label_data)\n",
    "\n",
    "# Turn shuffle to False if you want to keep the predefined order of the data\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=False)  # You can adjust the batch size as needed\n",
    "# i= 0\n",
    "# #----------------------Example to check batches---------------------\n",
    "# for i, (batch_features, batch_labels) in enumerate(dataloader):\n",
    "#     print(\"Batch\", i)\n",
    "#     print(\"Features Batch Shape:\", batch_features[20,20:36,:])  # (batch_size, sequence_length, channel_size)\n",
    "#     print(\"Labels Batch Shape:\", batch_labels[20])  # (batch_size, 1)\n",
    "#     i+=1\n",
    "#     if i == 2:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple 1D CNN model\n",
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, sequence_length):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels=5, out_channels=16, kernel_size=3, padding=1)  # Update in_channels to 5\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "#         # Dynamically compute the size of the flattened feature map\n",
    "#         self.flattened_size = 16 * (sequence_length // 2)  # After one max pooling (sequence_length // 2)\n",
    "#         self.fc1 = nn.Linear(self.flattened_size, 1)  # Fully connected layer\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = x.permute(0, 2, 1)  # Change shape to (batch_size, channels, sequence_length)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.pool(x)\n",
    "#         x = x.view(x.size(0), -1)  # Flatten\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "# class OptimizedCNN(nn.Module):\n",
    "#     def __init__(self, sequence_length):\n",
    "#         super(OptimizedCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels=5, out_channels=16, kernel_size=3, padding=1)  # 卷积层\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "#         # 计算全连接层输入的维度\n",
    "#         self.flattened_size = 16 * sequence_length  # 无池化层时，序列长度不变\n",
    "#         self.fc1 = nn.Linear(self.flattened_size, 1)  # 全连接层\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.permute(0, 2, 1)  # 调整维度为 (batch_size, channels, sequence_length)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = x.view(x.size(0), -1)  # 展平\n",
    "#         x = self.fc1(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalCNN(nn.Module):\n",
    "    def __init__(self, sequence_length, kernel_size=21):\n",
    "        super(LocalCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=5, out_channels=32, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hardtanh = nn.Hardtanh(min_val=0, max_val=1)\n",
    "        # self.clipper = nn.clippedReLU()\n",
    "        self.fc1 = nn.Linear(32 * sequence_length, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, channels, sequence_length)\n",
    "        x = self.conv1(x)  # Apply convolution\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x) # Apply convolution\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)  # Apply convolution\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten for fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, sequence_length):\n",
    "        super(MLP, self).__init__()\n",
    "        input_size = sequence_length * 5  # Flattened input size\n",
    "        hidden_size = 512  # Number of neurons in each hidden layer\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),  # Add dropout for regularization\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, 1),  # Final output layer\n",
    "            nn.Hardtanh(min_val=0, max_val=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten input to (batch_size, sequence_length * 5)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2292\n",
      "Batch 0\n",
      "Features Batch Shape: torch.Size([64, 56, 5])\n",
      "Labels Batch Shape: torch.Size([64, 1])\n",
      "Batch 1\n",
      "Features Batch Shape: torch.Size([64, 56, 5])\n",
      "Labels Batch Shape: torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "#------------------split train&validation dataset---------------------\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size) \n",
    "\n",
    "val_size = dataset_size - train_size  \n",
    "print(val_size)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_size, shuffle=False, drop_last=True)    \n",
    "\n",
    "for i, train_data in enumerate(train_dataloader):\n",
    "    print(\"Batch\", i)\n",
    "    channel = train_data[0].shape[2]\n",
    "    print(\"Features Batch Shape:\", train_data[0].shape)  # (batch_size, sequence_length, channel_size)\n",
    "    print(\"Labels Batch Shape:\", train_data[1].shape)  # (batch_size, 1)\n",
    "    i+=1\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torchsummary import summary\n",
    "import os\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "save_dir = f'D:/01IscBML/logfile/{timestamp}/'\n",
    "os.mkdir(save_dir)\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LocalCNN(sequence_length = sequence_length)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Training and validation setup\n",
    "num_epochs = 100  # Adjust as needed\n",
    "\n",
    "# Function to calculate Pearson correlation coefficient\n",
    "def pearson_correlation(predicted, actual):\n",
    "    pred_mean = predicted.mean()\n",
    "    actual_mean = actual.mean()\n",
    "    covariance = ((predicted - pred_mean) * (actual - actual_mean)).sum()\n",
    "    pred_std = ((predicted - pred_mean) ** 2).sum().sqrt()\n",
    "    actual_std = ((actual - actual_mean) ** 2).sum().sqrt()\n",
    "    return (covariance / (pred_std * actual_std)).item()\n",
    "\n",
    "plot_loss = []\n",
    "plot_pearson = []\n",
    "max_r = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch == 0:  # Save the architecture only in the first epoch\n",
    "        ## for MLP\n",
    "        input_size = sequence_length * channel\n",
    "        model_architecture = summary(model, input_size=(sequence_length, channel))\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    train_pearson = 0.0  # To accumulate Pearson correlation coefficient for training\n",
    "    \n",
    "    # Training loop\n",
    "    for batch_features, batch_labels in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Move data to appropriate device (e.g., GPU if available)\n",
    "        batch_features, batch_labels = batch_features.to('cpu'), batch_labels.to('cpu')\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate Pearson correlation for the batch\n",
    "        predicted = outputs.squeeze(1)  # Remove singleton dimension\n",
    "        actual = batch_labels.squeeze(1)\n",
    "        train_pearson += pearson_correlation(predicted, actual)\n",
    "\n",
    "    # Compute average training loss and Pearson correlation for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    plot_loss.append(epoch_loss)\n",
    "    epoch_train_pearson = train_pearson / len(train_dataloader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_pearson = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_features, val_labels in val_dataloader:  # Use a separate validation DataLoader\n",
    "            val_features, val_labels = val_features.to('cpu'), val_labels.to('cpu')\n",
    "            \n",
    "            val_outputs = model(val_features)\n",
    "            predicted = val_outputs.squeeze(1)\n",
    "            # print(predicted)\n",
    "            actual = val_labels.squeeze(1)\n",
    "            # print(actual)\n",
    "\n",
    "            # Calculate Pearson correlation for the validation set\n",
    "            val_pearson += pearson_correlation(predicted, actual)\n",
    "    \n",
    "    # Compute average validation Pearson correlation for the epoch\n",
    "    epoch_val_pearson = val_pearson / len(val_dataloader)\n",
    "    plot_pearson.append(epoch_val_pearson)\n",
    "\n",
    "    ##save the model and architecture\n",
    "    if epoch_val_pearson > max_r:\n",
    "        max_r = epoch_val_pearson\n",
    "        model_name = model.__class__.__name__\n",
    "        save_model_path = f'{save_dir}{model_name}.pth'\n",
    "        model_architecture = summary(model, input_size=(sequence_length,channel))\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    # 'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                    'architecture': model_architecture,\n",
    "                    }, save_model_path)\n",
    "          \n",
    "    print(f\"Model saved with Pearson correlation: {max_r:.4f}\")\n",
    "\n",
    "    # Print training and validation metrics\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    print(f\"Training Loss: {epoch_loss:.4f}\")\n",
    "    print(f\"Validation Pearson Correlation: {epoch_val_pearson:.4f}\")\n",
    "\n",
    "plt.figure()\n",
    "# plt.switch_backend('agg')\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# model.predict()\n",
    "\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Training Loss', color='r')\n",
    "ax1.plot(range(num_epochs), plot_loss, 'r', label='Training loss')\n",
    "ax1.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Validation Pearson Correlation', color='b')\n",
    "ax2.plot(range(num_epochs), plot_pearson, 'b', label='Validation Pearson Correlation')\n",
    "ax2.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "# plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "file_path = os.path.join(save_dir, 'loss_plot_MLP.png')\n",
    "plt.savefig(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2097\n"
     ]
    }
   ],
   "source": [
    "#------------------test dataset---------------------\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "test_label_data = FeatureProcessor(file_path='D:/01IscBML/',file_name='./test_reads_del50_keep1700.xlsx').labels_tensor\n",
    "test_combined_features = FeatureProcessor(file_path='D:/01IscBML/',file_name='./test_reads_del50_keep1700.xlsx').combined_features\n",
    "#---------------------Create dataset and dataloader-----------------\n",
    "dataset = TensorDataset(test_combined_features, test_label_data)\n",
    "testset_length = len(dataset)\n",
    "print(testset_length)\n",
    "\n",
    "# Turn shuffle to False if you want to keep the predefined order of the data\n",
    "test_dataloader = DataLoader(dataset, batch_size=testset_length, shuffle=False, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Pearson Correlation: 0.9379265308380127\n",
      "Test results saved to 'test_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "###---------test the model----------------\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables for tracking\n",
    "test_results = []  # To store results for each test sample\n",
    "test_pearson = 0.0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for test_features, test_labels in test_dataloader:  # Use the test DataLoader\n",
    "        # Move data to the appropriate device (e.g., CPU)\n",
    "        test_features, test_labels = test_features.to('cpu'), test_labels.to('cpu')\n",
    "        \n",
    "        # Perform predictions\n",
    "        test_outputs = model(test_features)\n",
    "        predicted = test_outputs.squeeze(1)  # Detach to numpy for processing\n",
    "        actual = test_labels.squeeze(1)\n",
    "\n",
    "        # Store each prediction with its actual label\n",
    "        for pred, act in zip(predicted, actual):\n",
    "            test_results.append({'Predicted': pred.item(), 'Actual': act.item()})\n",
    "        \n",
    "        # Calculate Pearson correlation for this batch\n",
    "        test_pearson += pearson_correlation(predicted, actual)\n",
    "\n",
    "# Compute average test Pearson correlation\n",
    "avg_test_pearson = test_pearson / len(test_dataloader)\n",
    "print(f\"Average Test Pearson Correlation: {avg_test_pearson}\")\n",
    "\n",
    "# Save test results to a DataFrame and export to a CSV file\n",
    "results_df = pd.DataFrame(test_results)\n",
    "test_result_path = f'{save_dir}{timestamp}.csv'\n",
    "test_result_path2 = f'{save_dir}{timestamp}_2.csv'\n",
    "results_df.to_csv(f'{save_dir}{timestamp}.csv', index=False)\n",
    "print(\"Test results saved to 'test_results.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "#output higher efficiency's pearson correlation\n",
    "\n",
    "#open the test result file\n",
    "test_result_2 = pd.read_csv(test_result_path)\n",
    "#filter all the actural value > 0.2\n",
    "test_result_2 = test_result_2[test_result_2['Actual'] > 0.2]\n",
    "test_result_2.to_csv(test_result_path2)\n",
    "\n",
    "predicted_values = test_result_2['Predicted']\n",
    "actual_values = test_result_2['Actual']\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "pearson_corr_2 = pearsonr(predicted_values, actual_values)\n",
    "pearson_correlation_2 = pearson_corr_2[0]\n",
    "print(pearson_correlation_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = str(summary(model, input_size=(sequence_length,channel)))\n",
    "print(model_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##draw the correlation\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.gridspec as gridspec\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "# Load first dataset\n",
    "test_results_1 = pd.read_csv(test_result_path)\n",
    "actual_1 = test_results_1['Actual'].values\n",
    "predicted_1 = test_results_1['Predicted'].values\n",
    "\n",
    "# Calculate density for first dataset\n",
    "xy_1 = np.vstack([actual_1, predicted_1])\n",
    "density_1 = gaussian_kde(xy_1)(xy_1)\n",
    "sorted_indices_1 = density_1.argsort()\n",
    "actual_sorted_1 = actual_1[sorted_indices_1]\n",
    "predicted_sorted_1 = predicted_1[sorted_indices_1]\n",
    "density_sorted_1 = density_1[sorted_indices_1]\n",
    "\n",
    "# Load second dataset\n",
    "test_results_2 = pd.read_csv(test_result_path2)\n",
    "actual_2 = test_results_2['Actual'].values\n",
    "predicted_2 = test_results_2['Predicted'].values\n",
    "\n",
    "# Calculate density for second dataset\n",
    "xy_2 = np.vstack([actual_2, predicted_2])\n",
    "density_2 = gaussian_kde(xy_2)(xy_2)\n",
    "sorted_indices_2 = density_2.argsort()\n",
    "actual_sorted_2 = actual_2[sorted_indices_2]\n",
    "predicted_sorted_2 = predicted_2[sorted_indices_2]\n",
    "density_sorted_2 = density_2[sorted_indices_2]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(21, 6))\n",
    "gs = gridspec.GridSpec(1, 3, width_ratios=[1.5, 1.5,1])\n",
    "\n",
    "# Plot first dataset\n",
    "scatter_1 = plt.subplot(gs[0, 0])\n",
    "scatter_1 = plt.scatter(\n",
    "    actual_sorted_1,\n",
    "    predicted_sorted_1,\n",
    "    c=density_sorted_1,\n",
    "    cmap='viridis',\n",
    "    s=5,\n",
    "    label='Data Points'\n",
    ")\n",
    "plt.colorbar(scatter_1, label='Density')\n",
    "plt.plot(\n",
    "    np.unique(actual_1),\n",
    "    np.poly1d(np.polyfit(actual_1, predicted_1, 1))(np.unique(actual_1)),\n",
    "    color='red',\n",
    "    linestyle='--',\n",
    "    label='Regression Line'\n",
    ")\n",
    "plt.title('Dataset 1')\n",
    "plt.xlim(-0.07, 1.2)\n",
    "plt.ylim(-0.07, 1.2)\n",
    "\n",
    "# Plot second dataset\n",
    "scatter_2 = plt.subplot(gs[0, 1])\n",
    "scatter_2 = plt.scatter(\n",
    "    actual_sorted_2,\n",
    "    predicted_sorted_2,\n",
    "    c=density_sorted_2,\n",
    "    cmap='viridis',\n",
    "    s=5,\n",
    "    label='Data Points'\n",
    ")\n",
    "plt.colorbar(scatter_2, label='Density')\n",
    "plt.plot(\n",
    "    np.unique(actual_2),\n",
    "    np.poly1d(np.polyfit(actual_2, predicted_2, 1))(np.unique(actual_2)),\n",
    "    color='red',\n",
    "    linestyle='--',\n",
    "    label='Regression Line'\n",
    ")\n",
    "plt.title('Dataset 2')\n",
    "plt.xlim(-0.07, 1.2)\n",
    "plt.ylim(-0.07, 1.2)\n",
    "\n",
    "# Show the plot\n",
    "\n",
    "\n",
    "\n",
    "#####-----------------add the model architecture-----------------\n",
    "buffer = io.StringIO()\n",
    "with contextlib.redirect_stdout(buffer):\n",
    "    summary(model, input_size=(sequence_length,channel))\n",
    "output = buffer.getvalue()\n",
    "\n",
    "ax_text = plt.subplot(gs[0,2])  # Use the second section of the grid\n",
    "ax_text.axis('off')  # Turn off the axes for the text area\n",
    "info_text = f\"Model: {model.__class__.__name__}\\n\" \\\n",
    "            f\"Architecture: {output}\\n\" \\\n",
    "            f\"Test Pearson Correlation: {avg_test_pearson:.4f}\\n\" \\\n",
    "            f\"Test Pearson Correlation2: {pearson_correlation_2:.4f}\\n\" \\\n",
    "            f\"Timestamp: {timestamp}\"\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0)\n",
    "plt.text(0.02, 0.98, info_text, transform=plt.gca().transAxes, fontsize=8, verticalalignment='top')\n",
    "\n",
    "save_path = os.path.join(save_dir, f\"{timestamp}_{avg_test_pearson:.4f}.png\")\n",
    "plt.savefig(save_dir+f'{timestamp}_{avg_test_pearson:.4f}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import contextlib\n",
    "import io\n",
    "with open(f\"{save_dir}\"+f\"{timestamp}.txt\", \"w\") as f:\n",
    "    buffer = io.StringIO()\n",
    "    with contextlib.redirect_stdout(buffer):\n",
    "        summary(model, input_size=(sequence_length,channel))\n",
    "    f.write(buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Initialize and train model\u001b[39;00m\n\u001b[0;32m     91\u001b[0m trainer \u001b[38;5;241m=\u001b[39m XGBoostTrainer(save_dir\u001b[38;5;241m=\u001b[39msave_dir)\n\u001b[0;32m     92\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m---> 93\u001b[0m     train_dataloader\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_dataloader\u001b[49m,\n\u001b[0;32m     94\u001b[0m     val_dataloader\u001b[38;5;241m=\u001b[39mval_dataloader\n\u001b[0;32m     95\u001b[0m )\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Predict using the trained model\u001b[39;00m\n\u001b[0;32m     98\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(xgb_model, test_dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "save_dir = f'D:/01IscBML/logfile/{timestamp}/'\n",
    "save_path = os.path.join(save_dir, f'predictions_{timestamp}.csv')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "class XGBoostTrainer:\n",
    "    def __init__(self,save_dir, params=None):\n",
    "        self.save_dir = save_dir\n",
    "        self.params = params or {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "            'max_depth': 6,\n",
    "            'eta': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'seed': 42\n",
    "        }\n",
    "\n",
    "    def train(self, train_dataloader, val_dataloader):\n",
    "        train_features, train_labels = self._extract_from_dataloader(train_dataloader)\n",
    "        val_features, val_labels = self._extract_from_dataloader(val_dataloader)\n",
    "        # Flatten features if necessary\n",
    "        train_features = train_features.reshape(train_features.shape[0], -1)\n",
    "        val_features = val_features.reshape(val_features.shape[0], -1)\n",
    "\n",
    "        # # Reshape labels\n",
    "        # train_labels = train_labels.ravel()\n",
    "        # val_labels = val_labels.ravel()\n",
    "\n",
    "        # Convert to DMatrix\n",
    "        dtrain = xgb.DMatrix(train_features, label=train_labels)\n",
    "        dval = xgb.DMatrix(val_features, label=val_labels)\n",
    "\n",
    "        # Train model\n",
    "        evals = [(dtrain, 'train'), (dval, 'eval')]\n",
    "        model = xgb.train(\n",
    "            self.params,\n",
    "            dtrain,\n",
    "            num_boost_round=100,\n",
    "            evals=evals,\n",
    "            verbose_eval=True\n",
    "        )\n",
    "\n",
    "        # Save model\n",
    "        timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "        save_path = os.path.join(self.save_dir, f'xgboost_model_{timestamp}.model')\n",
    "        model.save_model(save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    def predict(self, model, test_dataloader):\n",
    "        test_features, test_labels = self._extract_from_dataloader(test_dataloader)\n",
    "        test_features = test_features.reshape(test_features.shape[0], -1)  # Flatten features\n",
    "\n",
    "        # Convert to DMatrix\n",
    "        dtest = xgb.DMatrix(test_features)\n",
    "        predictions = model.predict(dtest)\n",
    "\n",
    "        # Save predictions and actual labels to an Excel file\n",
    "        timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'Actual': test_labels.ravel(),\n",
    "            'Predicted': predictions\n",
    "        })\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Predictions saved to {save_path}\")\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def _extract_from_dataloader(self, dataloader):\n",
    "        features = []\n",
    "        labels = []\n",
    "        for batch in dataloader:\n",
    "            features.append(batch[0].numpy())\n",
    "            labels.append(batch[1].numpy())\n",
    "        return np.vstack(features), np.vstack(labels)\n",
    "    ###chatgpt previously mistook the vstack as hstack, which cannot work because of wrong alignment.\n",
    "    ##but why hstack will cause misalignment?\n",
    "# Example usage\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Initialize and train model\n",
    "trainer = XGBoostTrainer(save_dir=save_dir)\n",
    "xgb_model = trainer.train(\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader\n",
    ")\n",
    "\n",
    "# Predict using the trained model\n",
    "test_predictions = trainer.predict(xgb_model, test_dataloader=test_dataloader)\n",
    "\n",
    "##draw the correlation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pearsonr\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43msave_path\u001b[49m)\n\u001b[0;32m     16\u001b[0m predicted \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     17\u001b[0m actual \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_path' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.gridspec as gridspec\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(save_path)\n",
    "predicted = data['Predicted'].values\n",
    "actual = data['Actual']\n",
    "pearson_r = pearsonr(actual, predicted)[0]\n",
    "print(f\"Pearson correlation: {pearson_r:.4f}\")\n",
    "\n",
    "# Calculate density using Gaussian KDE\n",
    "xy = np.vstack([actual, predicted])\n",
    "density = gaussian_kde(xy)(xy)\n",
    "\n",
    "# Sort points by density for better visualization\n",
    "sorted_indices = density.argsort()\n",
    "actual_sorted = actual[sorted_indices]\n",
    "predicted_sorted = predicted[sorted_indices]\n",
    "density_sorted = density[sorted_indices]\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(\n",
    "    actual_sorted,\n",
    "    predicted_sorted,\n",
    "    c=density_sorted,\n",
    "    cmap='viridis',\n",
    "    s=10\n",
    ")\n",
    "plt.colorbar(scatter, label='Density')\n",
    "plt.plot(\n",
    "    np.unique(actual),\n",
    "    np.poly1d(np.polyfit(actual, predicted, 1))(np.unique(actual)),\n",
    "    color='red',\n",
    "    linestyle='--',\n",
    "    label='Regression Line'\n",
    ")\n",
    "plt.title('Scatter Plot with Gaussian KDE Density')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.legend()\n",
    "plt.xlim(-0.1, 1.1)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
